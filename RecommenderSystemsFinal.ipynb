{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the top-10 recommendations for user #100 using user-user collaborative filtering with Pearson correlation and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YcaYI4_GQQDB",
    "outputId": "9aff5150-88dc-4c28-b92e-e587cbbcced6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x11473c290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use suprise for abstraction, load dataset and KNNBasic.\n",
    "from surprise import KNNBasic\n",
    "from surprise import Dataset\n",
    "\n",
    "# Load the movielens-100k dataset.\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Retrieve the trainset.\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Build an algorithm, and train it.\n",
    "algo = KNNBasic()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = str(100)  # raw user id (as in the ratings file). They are **strings**!\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "for movie in range(1, 9000):\n",
    "    pred = algo.predict(uid, str(movie), r_ui=4, verbose=False)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort good results for top recommendation, gets impossible past 1683.\n",
    "results = []\n",
    "\n",
    "for prediction in predictions:\n",
    "    if prediction[4] == True:\n",
    "        continue\n",
    "    else:\n",
    "        results.append((prediction.est,prediction.iid,))\n",
    "        \n",
    "results.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: Estimate: 5, Movie ID: 814\n",
      "#2: Estimate: 5, Movie ID: 1653\n",
      "#3: Estimate: 5, Movie ID: 1599\n",
      "#4: Estimate: 5, Movie ID: 1536\n",
      "#5: Estimate: 5, Movie ID: 1293\n",
      "#6: Estimate: 5, Movie ID: 1201\n",
      "#7: Estimate: 5, Movie ID: 1189\n",
      "#8: Estimate: 5, Movie ID: 1122\n",
      "#9: Estimate: 4.999999999999999, Movie ID: 1500\n",
      "#10: Estimate: 4.999999999999999, Movie ID: 1467\n"
     ]
    }
   ],
   "source": [
    "# Print out the top results\n",
    "for count, topResult in enumerate(results[0:10]):\n",
    "    print(\"#{}: Estimate: {}, Movie ID: {}\".format(count + 1, topResult[0], topResult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or I could use the packages method which I just found now of course\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    if uid == 100:\n",
    "        print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your recommendation system using Precision and Recall at 10 and neighborhood size from 2 to 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is with recall at 10 , neighborhood size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.6955268490374873\n",
      "0.4700381187105888\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.6974402442521468\n",
      "0.46978343882498513\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.7058202465383312\n",
      "0.4684097867860941\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.6889188618709896\n",
      "0.48223750100610624\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.6848758388694488\n",
      "0.4635279339983386\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "kf = KFold(n_splits=5)\n",
    "algo = KNNBasic(k=2)\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is with recall at 10 and neighbors at 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.8398917058853291\n",
      "0.3181896118169244\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.8275332862024268\n",
      "0.3138428459632695\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.8383212139574805\n",
      "0.33578373344271184\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.8467886693684564\n",
      "0.3232230615263422\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.8298899673103491\n",
      "0.32518367526955416\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.load_builtin('ml-100k')\n",
    "kf = KFold(n_splits=5)\n",
    "algo = KNNBasic(k=50)\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the top-10 similar movies to (Toy Story) and (Batman Forever)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we do Toy Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "The 10 nearest neighbors of Toy Story are:\n",
      "Beauty and the Beast (1991)\n",
      "Raiders of the Lost Ark (1981)\n",
      "That Thing You Do! (1996)\n",
      "Lion King, The (1994)\n",
      "Craft, The (1996)\n",
      "Liar Liar (1997)\n",
      "Aladdin (1992)\n",
      "Cool Hand Luke (1967)\n",
      "Winnie the Pooh and the Blustery Day (1968)\n",
      "Indiana Jones and the Last Crusade (1989)\n"
     ]
    }
   ],
   "source": [
    "import io  # needed because of weird encoding of u.item file\n",
    "\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import get_dataset_dir\n",
    "\n",
    "\n",
    "def read_item_names():\n",
    "    \"\"\"Read the u.item file from MovieLens 100-k dataset and return two\n",
    "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "\n",
    "# First, train the algortihm to compute the similarities between items\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Read the mappings raw id <-> movie name\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# Retrieve inner id of the movie Toy Story\n",
    "toy_story_raw_id = name_to_rid['Toy Story (1995)']\n",
    "toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n",
    "\n",
    "# Retrieve inner ids of the nearest neighbors of Toy Story.\n",
    "toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n",
    "\n",
    "# Convert inner ids of the neighbors into names.\n",
    "toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in toy_story_neighbors)\n",
    "toy_story_neighbors = (rid_to_name[rid]\n",
    "                       for rid in toy_story_neighbors)\n",
    "\n",
    "print()\n",
    "print('The 10 nearest neighbors of Toy Story are:')\n",
    "for movie in toy_story_neighbors:\n",
    "    print(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we do Batman Forever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 nearest neighbors of Batman Forever are:\n",
      "Net, The (1995)\n",
      "Cape Fear (1991)\n",
      "Liar Liar (1997)\n",
      "Mission: Impossible (1996)\n",
      "Interview with the Vampire (1994)\n",
      "Batman & Robin (1997)\n",
      "Casper (1995)\n",
      "Batman Returns (1992)\n",
      "Conspiracy Theory (1997)\n",
      "Junior (1994)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve inner id of the movie Toy Story\n",
    "batman_raw_id = name_to_rid['Batman Forever (1995)']\n",
    "batman_inner_id = algo.trainset.to_inner_iid(batman_raw_id)\n",
    "\n",
    "# Retrieve inner ids of the nearest neighbors of Toy Story.\n",
    "batman_neighbors = algo.get_neighbors(batman_inner_id, k=10)\n",
    "\n",
    "# Convert inner ids of the neighbors into names.\n",
    "batman_neighbors = (algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in batman_neighbors)\n",
    "batman_neighbors = (rid_to_name[rid]\n",
    "                       for rid in batman_neighbors)\n",
    "\n",
    "print()\n",
    "print('The 10 nearest neighbors of Batman Forever are:')\n",
    "for movie in batman_neighbors:\n",
    "    print(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
